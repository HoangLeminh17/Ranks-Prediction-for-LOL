{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg_Kills</th>\n",
       "      <th>Avg_Deaths</th>\n",
       "      <th>Avg_Assists</th>\n",
       "      <th>Avg_total_gold</th>\n",
       "      <th>Avg_total_minion</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.92</td>\n",
       "      <td>4.15</td>\n",
       "      <td>7.31</td>\n",
       "      <td>12732.31</td>\n",
       "      <td>114.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.15</td>\n",
       "      <td>4.85</td>\n",
       "      <td>6.50</td>\n",
       "      <td>9208.70</td>\n",
       "      <td>131.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.50</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.50</td>\n",
       "      <td>12305.95</td>\n",
       "      <td>164.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.70</td>\n",
       "      <td>5.95</td>\n",
       "      <td>6.55</td>\n",
       "      <td>11465.35</td>\n",
       "      <td>152.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.25</td>\n",
       "      <td>7.90</td>\n",
       "      <td>6.15</td>\n",
       "      <td>12844.50</td>\n",
       "      <td>155.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>7.60</td>\n",
       "      <td>6.85</td>\n",
       "      <td>8.35</td>\n",
       "      <td>11153.65</td>\n",
       "      <td>33.45</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>6.45</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>11494.40</td>\n",
       "      <td>172.30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>5.90</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.90</td>\n",
       "      <td>10457.45</td>\n",
       "      <td>124.25</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>7.50</td>\n",
       "      <td>4.90</td>\n",
       "      <td>8.00</td>\n",
       "      <td>11815.15</td>\n",
       "      <td>182.20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>5.00</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.30</td>\n",
       "      <td>9553.45</td>\n",
       "      <td>126.35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avg_Kills  Avg_Deaths  Avg_Assists  Avg_total_gold  Avg_total_minion  \\\n",
       "0         9.92        4.15         7.31        12732.31            114.54   \n",
       "1         4.15        4.85         6.50         9208.70            131.40   \n",
       "2         6.50        9.00         8.50        12305.95            164.05   \n",
       "3         6.70        5.95         6.55        11465.35            152.70   \n",
       "4        10.25        7.90         6.15        12844.50            155.60   \n",
       "..         ...         ...          ...             ...               ...   \n",
       "866       7.60        6.85         8.35        11153.65             33.45   \n",
       "867       6.45        6.00         8.00        11494.40            172.30   \n",
       "868       5.90        5.65         5.90        10457.45            124.25   \n",
       "869       7.50        4.90         8.00        11815.15            182.20   \n",
       "870       5.00        5.90         7.30         9553.45            126.35   \n",
       "\n",
       "     Rank  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "..    ...  \n",
       "866     8  \n",
       "867     8  \n",
       "868     8  \n",
       "869     8  \n",
       "870     8  \n",
       "\n",
       "[871 rows x 6 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_data=pd.read_csv('../data.csv')\n",
    "rank_data = rank_data.drop(columns=['Avg_KDA']) # drop Avg_KDA to avoid multicolinearity\n",
    "rank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=rank_data.iloc[:,:5].to_numpy() # X takes the first 5 features - columns\n",
    "y=rank_data.iloc[:,-1].to_numpy() # y is the label - the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, shuffle = True, random_state = 42, stratify = y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.0000e-01 2.8000e+00 3.5000e+00 6.7381e+03 1.5800e+01]\n",
      "[1.33000e+01 1.04500e+01 1.90500e+01 1.55536e+04 2.21550e+02]\n",
      "Saved data!\n"
     ]
    }
   ],
   "source": [
    "# For deployment - clipping input\n",
    "X_min = X_train.min(axis=0) # Take the min value of every columns\n",
    "X_max = X_train.max(axis=0) # Take the max value of every columns\n",
    "print(X_min)\n",
    "print(X_max)\n",
    "np.save('data_min.npy', X_min)\n",
    "np.save('data_max.npy', X_max)\n",
    "\n",
    "print(\"Saved data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[-0.81531008  0.51720542  0.66100031  0.0430277   0.2253418 ]\n",
      " [ 1.50241924 -0.20879709 -0.48590363  1.08551027  1.13907225]\n",
      " [ 0.39085517  2.16026374 -0.29151313  0.85538548  0.10440688]\n",
      " ...\n",
      " [-0.93356158 -1.16406356 -0.29151313 -0.6696478   0.15367666]\n",
      " [-2.09242624 -0.667325    1.36080611 -1.88118333 -1.28141764]\n",
      " [-1.64780062  0.48663689  0.25278027 -2.16819527 -1.36132426]]\n",
      "\n",
      "\n",
      "X_test: [[-8.21508094e-02 -8.20167631e-01  7.77634613e-01 -5.02837773e-01\n",
      "  -7.79761703e-01]\n",
      " [ 9.11161757e-01  2.19847440e+00  3.30536466e-01  1.03526860e+00\n",
      "   3.99082639e-02]\n",
      " [-1.95052445e+00 -9.73010266e-01  1.03034226e+00 -2.35902389e+00\n",
      "  -1.06463061e+00]\n",
      " [ 6.98309065e-01 -8.58378290e-01  2.33341216e-01  6.18403738e-01\n",
      "   9.97533614e-01]\n",
      " [-3.42304101e-01 -1.20227422e+00  3.89507170e-02 -1.55279499e-01\n",
      "  -9.17717086e-01]\n",
      " [ 5.97509858e-02 -1.77438001e-02 -8.55245579e-01  7.60308330e-02\n",
      "   1.01813734e+00]\n",
      " [ 7.45609663e-01  1.66352518e+00  2.52780266e-01  1.75717216e+00\n",
      "   3.12235772e-01]\n",
      " [-4.84205896e-01 -5.52693021e-01 -7.77489379e-01 -7.63498468e-01\n",
      "   4.66315810e-01]\n",
      " [-4.60555597e-01  4.78994762e-01  9.72025113e-01  6.88575455e-01\n",
      "   5.62167926e-01]\n",
      " [ 2.61398330e+00  8.99312007e-01 -6.80294130e-01  1.38295504e+00\n",
      "   6.14125148e-01]\n",
      " [ 9.82112655e-01  6.31837397e-01  2.52780266e-01  5.45316209e-01\n",
      "   4.00025561e-01]\n",
      " [ 1.14766475e+00  2.49730810e-01 -7.77489379e-01  1.12495382e+00\n",
      "  -1.23214786e+00]\n",
      " [ 6.74658765e-01  2.80984494e+00  5.83897669e-02  5.52269297e-01\n",
      "   3.08652515e-01]\n",
      " [ 6.03707868e-01 -1.58438080e+00 -5.24781730e-01  5.77486257e-01\n",
      "   1.30927695e+00]\n",
      " [-6.73408290e-01 -7.81956973e-01  1.36145967e-01 -5.69933470e-01\n",
      "  -5.69245373e-01]\n",
      " [ 6.03707868e-01  2.04563177e+00  2.33341216e-01  5.68033902e-01\n",
      "  -8.59489165e-01]\n",
      " [-5.31506494e-01  1.35098834e-01 -1.12739228e+00 -7.03419942e-01\n",
      "   5.93521422e-01]\n",
      " [-1.54846936e+00 -1.20227422e+00  7.19317464e-01 -2.36677802e+00\n",
      "  -1.68632565e+00]\n",
      " [-9.33561581e-01  9.75733324e-01 -1.36066088e+00 -1.01964126e+00\n",
      "   2.19071098e-01]\n",
      " [-3.89604699e-01 -1.27869553e+00 -7.38611279e-01  2.68153946e-01\n",
      "   1.12832248e+00]\n",
      " [ 1.07051584e-01  5.17205421e-01 -4.08147431e-01  1.85388057e-03\n",
      "   5.08419076e-01]\n",
      " [-3.89604699e-01 -8.96588949e-01 -4.08147431e-01 -7.14153972e-01\n",
      "   6.06958635e-01]\n",
      " [-6.02457392e-01  1.09036530e+00 -3.88708381e-01 -2.67265870e-01\n",
      "   1.43822708e-01]\n",
      " [-9.80862179e-01  6.70048055e-01  1.76902616e+00 -1.06780020e+00\n",
      "  -8.89051032e-01]\n",
      " [ 3.43554576e-01 -9.41651174e-02 -8.35806529e-01 -3.57976433e-01\n",
      "   5.86354909e-01]\n",
      " [ 2.96253978e-01  4.78994762e-01 -1.12739228e+00  3.90041258e-01\n",
      "   1.11219783e+00]\n",
      " [-1.21736517e+00  4.40784103e-01  2.02173381e+00 -1.64340054e+00\n",
      "  -1.63526424e+00]\n",
      " [ 1.47876894e+00 -1.96648739e+00 -9.71879878e-01  3.89624714e-01\n",
      "   9.20493595e-01]\n",
      " [ 2.01652781e-01  2.11520152e-01  8.55390813e-01  1.02363740e+00\n",
      "   6.93852609e-01]\n",
      " [-3.89604699e-01 -3.99850386e-01  6.41561264e-01 -5.19659760e-01\n",
      "   3.18506471e-01]\n",
      " [-9.57211880e-01 -6.29114338e-01 -7.77489379e-01 -1.18613087e+00\n",
      "   2.10112957e-01]\n",
      " [-4.13254998e-01 -3.99850386e-01 -4.27586481e-01 -3.48395911e-01\n",
      "  -7.60053791e-01]\n",
      " [ 1.02941325e+00  9.75733324e-01  7.38756514e-01  1.23863841e+00\n",
      "   6.31994324e-02]\n",
      " [-1.11999118e-02  6.31837397e-01  2.72219316e-01  2.55561487e-01\n",
      "   3.31047870e-01]\n",
      " [ 7.69259962e-01  6.31837397e-01 -9.91318928e-01  1.26721087e-01\n",
      "  -1.59495260e+00]\n",
      " [-4.13254998e-01  3.26152128e-01  6.80439364e-01 -1.56272798e-01\n",
      "  -1.43191442e+00]\n",
      " [ 8.87511458e-01  1.31962925e+00 -3.30391231e-01  5.05616320e-01\n",
      "   2.36091568e-01]\n",
      " [-9.80862179e-01  3.64362786e-01  8.55390813e-01 -1.16434239e+00\n",
      "  -6.53451905e-01]\n",
      " [ 1.24503874e-02  5.55416079e-01  2.52780266e-01  4.20288793e-01\n",
      "   1.17669645e+00]\n",
      " [-9.33561581e-01 -1.32375776e-01  3.89507170e-02 -9.62798962e-01\n",
      "  -7.77074261e-01]\n",
      " [ 8.34012850e-02  4.40784103e-01 -5.63659830e-01  7.55030314e-01\n",
      "   1.18744622e+00]\n",
      " [ 9.34812057e-01 -6.67324997e-01 -6.21976980e-01  1.37068300e+00\n",
      "   1.62191609e+00]\n",
      " [ 7.92910261e-01  1.81636781e+00 -5.82445326e-02  2.73761275e-01\n",
      "   4.05400446e-01]\n",
      " [-3.48502110e-02 -6.67324997e-01 -1.18570943e+00  1.60973857e-01\n",
      "   8.55099160e-01]\n",
      " [ 1.45511864e+00 -4.38061045e-01 -4.27586481e-01  1.35809054e+00\n",
      "   1.69268541e+00]\n",
      " [ 4.14505474e-01  2.08384242e+00 -3.30391231e-01  7.12414614e-01\n",
      "   5.38876758e-01]\n",
      " [ 3.90855175e-01 -9.73010266e-01 -8.94123679e-01  3.54026185e-01\n",
      "   9.84096401e-01]\n",
      " [-5.85005102e-02 -9.41651174e-02 -2.33195982e-01  1.07935651e-02\n",
      "   8.10308452e-01]\n",
      " [-4.60555597e-01 -3.23429069e-01  2.52780266e-01 -9.54573093e-02\n",
      "   9.67075932e-01]\n",
      " [ 3.67204876e-01 -2.08797093e-01 -1.04963608e+00 -7.55272599e-02\n",
      "   7.26997734e-01]\n",
      " [-5.85005102e-02 -9.34799607e-01 -1.12739228e+00 -6.66731990e-01\n",
      "  -1.27608986e-01]\n",
      " [-4.60555597e-01  2.04668585e-02 -2.72074082e-01 -4.88515053e-01\n",
      "   6.86686096e-01]\n",
      " [-1.92687415e+00 -7.81956973e-01  1.47744041e+00 -1.60866714e+00\n",
      "  -1.47491350e+00]\n",
      " [ 5.97509858e-02 -3.99850386e-01 -7.38611279e-01 -4.25168256e-01\n",
      "  -7.77074261e-01]\n",
      " [-5.55156794e-01  1.66352518e+00 -9.52440828e-01 -9.33769019e-01\n",
      "  -3.09459263e-01]\n",
      " [ 2.25303080e-01  1.73309493e-01 -9.13562729e-01  2.49056986e-01\n",
      "  -1.09692703e-01]\n",
      " [-2.24052605e-01 -3.61639728e-01 -4.85903631e-01 -6.50198380e-01\n",
      "   5.01252562e-01]\n",
      " [ 6.03707868e-01 -2.85218410e-01  2.91658366e-01  6.25132532e-01\n",
      "   2.33404125e-01]\n",
      " [-1.83227295e+00 -2.08797093e-01  2.70210055e+00 -1.56400076e+00\n",
      "  -1.61913958e+00]\n",
      " [ 9.34812057e-01  6.70048055e-01  5.83897669e-02  1.32572824e+00\n",
      "  -1.58066668e-01]\n",
      " [ 3.19904277e-01 -1.32375776e-01  7.78288168e-02  4.07824502e-01\n",
      "   5.47834899e-01]\n",
      " [-1.54846936e+00 -7.05535655e-01  1.36145967e-01 -1.88845684e+00\n",
      "  -3.55145786e-01]\n",
      " [-1.11999118e-02  1.73309493e-01  2.52780266e-01  5.83606256e-01\n",
      "  -1.11484331e-01]\n",
      " [-4.60555597e-01 -1.04943158e+00 -2.52635032e-01 -5.56572006e-01\n",
      "   2.39674824e-01]\n",
      " [ 2.01652781e-01  1.01394398e+00 -3.49830281e-01  8.45740877e-01\n",
      "  -2.89751351e-01]\n",
      " [-1.40656757e+00 -1.04943158e+00  1.14697656e+00 -1.38937251e+00\n",
      "  -4.49206274e-01]\n",
      " [-1.95052445e+00  6.70048055e-01  1.47744041e+00 -1.30436540e+00\n",
      "  -1.54478701e+00]\n",
      " [ 7.69259962e-01  1.47247189e+00 -8.35806529e-01  6.17474523e-01\n",
      "  -1.18556552e+00]\n",
      " [ 1.02941325e+00  2.92447691e+00 -6.21976980e-01  1.14815214e+00\n",
      "  -4.50997902e-01]\n",
      " [ 7.21959364e-01  1.09036530e+00 -5.24781730e-01  8.63139618e-01\n",
      "   6.75936326e-01]\n",
      " [ 5.32756970e-01  3.64362786e-01  4.27731715e-01  1.20134166e+00\n",
      "   1.16146761e+00]\n",
      " [-1.99782505e+00 -5.59544587e-02  1.34136706e+00 -2.17074580e+00\n",
      "  -1.58151539e+00]\n",
      " [ 7.92910261e-01  1.62531452e+00 -2.91513131e-01  3.23746608e-01\n",
      "  -3.57833228e-01]\n",
      " [-2.24052605e-01  5.86775172e-02  7.77634613e-01 -4.15844068e-01\n",
      "  -6.74951445e-01]\n",
      " [-1.02816278e+00 -1.12585290e+00 -9.71879878e-01 -8.49370706e-01\n",
      "   5.12002332e-01]\n",
      " [ 2.48953380e-01  3.26152128e-01 -5.82445326e-02  1.03184012e+00\n",
      "   1.46873187e+00]\n",
      " [ 2.61398330e+00 -3.61639728e-01 -5.44220780e-01  1.19544596e+00\n",
      "   7.28789362e-01]\n",
      " [ 1.76257253e+00 -7.05535655e-01  4.47170765e-01  1.60827354e+00\n",
      "   7.77163327e-01]\n",
      " [ 1.21861565e+00 -8.96588949e-01 -7.58050329e-01  6.00620494e-01\n",
      "   2.45945523e-01]\n",
      " [ 7.92910261e-01  1.70173584e+00 -3.88054827e-02  3.61011315e-01\n",
      "   8.73864150e-02]\n",
      " [-1.19371487e+00 -1.32375776e-01  1.61351376e+00 -8.34855735e-01\n",
      "  -8.53218465e-01]\n",
      " [-5.07856195e-01 -1.46974883e+00 -3.10952181e-01  4.55887322e-01\n",
      "   1.37198394e+00]\n",
      " [-5.85005102e-02  4.02573445e-01  1.10809846e+00 -7.38931240e-02\n",
      "  -4.81455584e-01]\n",
      " [-2.23432804e+00 -1.66080212e+00  1.98285571e+00 -2.49686806e+00\n",
      "  -1.45072652e+00]\n",
      " [ 8.16560561e-01 -7.05535655e-01 -3.69269331e-01  2.98049020e-01\n",
      "  -4.77872327e-01]\n",
      " [-1.53101707e-01 -1.16406356e+00  1.49687946e+00 -1.16348615e-01\n",
      "   5.46043271e-01]\n",
      " [-4.13254998e-01 -1.31690619e+00 -7.96928429e-01  1.50816581e-01\n",
      "   1.19550855e+00]\n",
      " [-1.97417475e+00  3.26152128e-01  2.83817390e+00 -1.85740825e+00\n",
      "  -1.50895444e+00]\n",
      " [-3.65954400e-01  4.02573445e-01  1.01090321e+00 -3.74221666e-01\n",
      "  -1.01983990e+00]\n",
      " [ 1.10036415e+00  1.09036530e+00  1.55585016e-01  9.86917401e-01\n",
      "   7.23414477e-01]\n",
      " [-4.84205896e-01 -9.73010266e-01 -9.13562729e-01 -3.53138109e-01\n",
      "   8.96306612e-01]\n",
      " [-5.85005102e-02 -1.12585290e+00 -1.01075798e+00  1.76321918e-01\n",
      "   1.23761181e+00]\n",
      " [-3.18653802e-01 -1.62259146e+00  1.67183091e+00 -2.63429739e-02\n",
      "   1.13365026e-01]\n",
      " [ 5.97509858e-02  1.77815716e+00 -3.88054827e-02  3.74084402e-01\n",
      "   3.52547410e-01]\n",
      " [-1.97417475e+00 -8.20167631e-01  2.39107576e+00 -2.20612004e+00\n",
      "  -1.68811727e+00]\n",
      " [ 1.54971984e+00 -9.34799607e-01 -5.83098880e-01  8.80314066e-01\n",
      "   9.59013604e-01]\n",
      " [-2.47702904e-01  2.11520152e-01 -1.59392948e+00  7.87040153e-01\n",
      "   1.08442759e+00]\n",
      " [ 1.43146834e+00  6.70048055e-01 -4.27586481e-01  1.35863525e+00\n",
      "   1.51885035e-01]\n",
      " [ 1.69162163e+00  6.31837397e-01 -4.08147431e-01  1.92888459e+00\n",
      "  -3.57833228e-01]\n",
      " [-8.38960384e-01 -2.85218410e-01  7.26171405e-05 -1.39611020e-01\n",
      "   6.94748423e-01]\n",
      " [-4.60555597e-01  1.28141859e+00 -1.10795323e+00  1.43575116e-01\n",
      "  -8.37140918e-02]\n",
      " [-3.65954400e-01 -7.43746314e-01  1.47744041e+00  2.32971962e-01\n",
      "  -1.52691783e-01]\n",
      " [ 1.99907552e+00  7.08258714e-01 -2.91513131e-01  1.45280634e+00\n",
      "  -8.94425917e-01]\n",
      " [-5.85005102e-02 -8.96588949e-01 -1.12739228e+00 -2.20837186e-01\n",
      "   1.73384576e-01]\n",
      " [ 1.83352343e+00  7.46469372e-01  3.89507170e-02  1.78235707e+00\n",
      "   1.16863412e+00]\n",
      " [-7.44359187e-01  1.12857596e+00 -1.10795323e+00 -7.88971762e-01\n",
      "  -2.98709493e-01]\n",
      " [-1.35926697e+00 -1.16406356e+00  2.33341216e-01 -8.58118140e-01\n",
      "   6.40999573e-01]\n",
      " [-1.29451408e-01 -1.77438001e-02 -1.01075798e+00 -3.85884911e-01\n",
      "   8.20115300e-02]\n",
      " [-2.25797834e+00 -9.34799607e-01  1.63295281e+00 -2.88354946e+00\n",
      "  -1.70961681e+00]\n",
      " [ 9.82112655e-01  1.39605057e+00 -9.13562729e-01  5.33204378e-01\n",
      "   6.30249803e-01]\n",
      " [ 3.61006866e-02 -9.41651174e-02 -1.43841708e+00 -5.60457968e-02\n",
      "   8.97202426e-01]\n",
      " [-4.60555597e-01 -9.41651174e-02 -9.13562729e-01 -8.82277717e-01\n",
      "  -1.85205127e+00]\n",
      " [-2.00402305e-01  9.75733324e-01  1.36145967e-01 -2.54865663e-01\n",
      "   1.04406884e-01]\n",
      " [-6.26107691e-01  1.39605057e+00  9.91464162e-01 -6.87655337e-01\n",
      "  -7.67220305e-01]\n",
      " [ 2.01652781e-01  1.58710386e+00 -9.71226324e-02  9.92396562e-01\n",
      "   9.43784763e-01]\n",
      " [-2.47702904e-01  8.99312007e-01  2.50771006e+00  4.45505753e-01\n",
      "  -1.23304367e+00]\n",
      " [ 1.00576295e+00  2.49730810e-01 -8.94123679e-01  3.47713934e-01\n",
      "   4.57357668e-01]\n",
      " [-1.61942026e+00 -1.32375776e-01  1.82734331e+00 -8.22775946e-01\n",
      "  -9.29362670e-01]\n",
      " [ 1.54352183e-01  1.73309493e-01 -5.24781730e-01 -7.91479923e-02\n",
      "  -1.33695812e+00]\n",
      " [-1.14641427e+00 -9.73010266e-01  1.94463116e-01 -1.34842299e+00\n",
      "  -1.70513774e+00]\n",
      " [-8.62610683e-01 -1.62259146e+00  1.28304991e+00 -1.25704844e-01\n",
      "  -1.44266419e+00]\n",
      " [-2.71353203e-01  4.78994762e-01  3.11097416e-01  4.99432237e-01\n",
      "   1.21252901e+00]\n",
      " [ 8.16560561e-01  1.93099979e+00 -7.76835825e-02  1.29797997e+00\n",
      "   9.16910338e-01]\n",
      " [-9.57211880e-01 -1.73722344e+00 -1.84663712e+00 -1.42807910e+00\n",
      "  -9.65195237e-01]\n",
      " [-1.64307056e+00  5.86775172e-02  1.34136706e+00 -1.55243364e+00\n",
      "  -1.22139809e+00]\n",
      " [ 2.01652781e-01 -6.29114338e-01 -1.16627038e+00  9.04345476e-01\n",
      "   1.19730017e+00]\n",
      " [-1.76752006e-01  1.35098834e-01 -5.44220780e-01  3.42619275e-01\n",
      "   1.12653085e+00]\n",
      " [-6.97058589e-01 -2.85218410e-01 -5.63659830e-01 -4.40067730e-01\n",
      "   3.90124498e-02]\n",
      " [ 1.54352183e-01 -1.77438001e-02 -1.22458753e+00  4.08913925e-01\n",
      "   1.07457363e+00]\n",
      " [-1.59576996e+00  2.87941469e-01  5.44366015e-01 -1.40651492e+00\n",
      "  -3.04084378e-01]\n",
      " [ 4.14505474e-01  1.35098834e-01 -5.63659830e-01  2.08588091e-01\n",
      "   2.26237612e-01]\n",
      " [-6.49757991e-01 -3.61639728e-01 -8.16367479e-01 -8.28639610e-01\n",
      "   6.05167006e-01]\n",
      " [ 8.16560561e-01  1.20499728e+00 -8.55245579e-01  1.10351780e+00\n",
      "   4.66315810e-01]\n",
      " [ 1.52606954e+00  2.04563177e+00  1.55585016e-01  1.43326079e+00\n",
      "   1.12473923e+00]\n",
      " [-6.02457392e-01  9.37522666e-01  3.69414565e-01 -4.42278620e-01\n",
      "   3.31047870e-01]\n",
      " [-1.66672086e+00 -5.90903680e-01 -1.73000282e+00 -2.46886345e+00\n",
      "  -4.43831389e-01]\n",
      " [-1.11999118e-02 -2.85218410e-01  1.16706917e-01  3.17882944e-01\n",
      "   7.10873079e-01]\n",
      " [-2.95003502e-01 -1.50795949e+00  7.78288168e-02  4.81905328e-01\n",
      "   1.31286020e+00]\n",
      " [-2.42353043e+00 -4.38061045e-01  4.27731715e-01 -3.25241559e+00\n",
      "  -1.73201217e+00]\n",
      " [-4.13254998e-01 -1.66080212e+00  8.55390813e-01  4.40347010e-01\n",
      "  -1.57882794e+00]\n",
      " [ 2.48953380e-01  9.68881758e-02  1.94463116e-01  1.87856995e-01\n",
      "   2.35195753e-01]\n",
      " [ 5.80057568e-01  6.70048055e-01 -3.30391231e-01  1.37404740e+00\n",
      "   1.94440919e+00]\n",
      " [ 1.80987313e+00  3.26837284e+00 -9.91318928e-01  1.91363266e+00\n",
      "   7.50288902e-01]\n",
      " [-6.26107691e-01 -1.70586435e-01 -7.58050329e-01 -7.02403497e-02\n",
      "   9.42888949e-01]\n",
      " [-1.78497235e+00  3.45942613e+00 -2.72074082e-01 -1.98852363e+00\n",
      "  -1.17302412e+00]\n",
      " [-2.47702904e-01 -1.77438001e-02  2.72219316e-01 -4.36895584e-01\n",
      "  -1.27514694e+00]\n",
      " [-7.68009487e-01 -2.57785793e+00 -4.27586481e-01 -1.00208231e+00\n",
      "   2.12800399e-01]\n",
      " [ 1.54352183e-01  2.54237033e+00  7.26171405e-05  7.98703397e-01\n",
      "   7.95079611e-01]\n",
      " [ 5.56407269e-01  3.26152128e-01 -5.05342680e-01  6.23169083e-02\n",
      "  -4.93101168e-01]\n",
      " [ 7.69259962e-01  5.17205421e-01 -1.74878832e-01  1.06939320e+00\n",
      "   5.40668386e-01]\n",
      " [ 4.38155773e-01 -8.58378290e-01 -1.06907513e+00  3.78826600e-01\n",
      "   1.42125372e+00]\n",
      " [-2.00402305e-01 -2.08797093e-01  7.19317464e-01 -1.76916663e-02\n",
      "  -9.06967316e-01]\n",
      " [ 2.72603679e-01  2.87941469e-01 -2.33195982e-01 -3.22377904e-01\n",
      "  -1.38358756e-01]\n",
      " [ 1.14766475e+00 -1.70586435e-01  1.55585016e-01  1.95339663e+00\n",
      "   1.94978408e+00]\n",
      " [ 1.78622283e+00  3.26152128e-01 -5.63659830e-01  1.09579571e+00\n",
      "  -7.68116119e-01]\n",
      " [-1.59576996e+00  5.55416079e-01 -5.82445326e-02 -8.58118140e-01\n",
      "   3.97338118e-01]\n",
      " [ 2.04637612e+00 -5.59544587e-02 -7.77489379e-01  5.39901131e-01\n",
      "   2.79986462e-01]\n",
      " [ 8.40210860e-01  5.17205421e-01  2.52780266e-01  1.51080214e+00\n",
      "  -1.15958691e+00]\n",
      " [ 1.07051584e-01  1.73309493e-01 -4.47025531e-01  3.92508483e-01\n",
      "   1.22507041e+00]\n",
      " [-1.76132205e+00 -8.96588949e-01  2.50771006e+00 -2.04885849e+00\n",
      "  -1.53851631e+00]\n",
      " [ 1.07051584e-01  1.43426123e+00  1.69126996e+00  3.82511416e-01\n",
      "  -7.96782172e-01]\n",
      " [ 6.27358167e-01  2.04668585e-02 -3.69269331e-01 -1.61655834e-01\n",
      "  -1.47133024e+00]\n",
      " [ 3.90855175e-01 -8.96588949e-01  6.02683164e-01  2.62377513e-02\n",
      "  -1.02790223e+00]\n",
      " [ 3.61006866e-02 -5.52693021e-01 -9.71226324e-02 -1.40123690e-01\n",
      "  -1.57076562e+00]\n",
      " [ 2.11732702e+00 -9.41651174e-02 -2.72074082e-01  1.51157115e+00\n",
      "   6.94748423e-01]\n",
      " [ 8.40210860e-01  2.04668585e-02 -8.35806529e-01  6.80203630e-02\n",
      "   3.45380897e-01]\n",
      " [ 9.58462356e-01  1.47247189e+00 -6.99733180e-01  3.27046922e-01\n",
      "   3.83900906e-01]\n",
      " [ 1.43146834e+00 -1.77438001e-02 -9.91318928e-01  1.26218919e+00\n",
      "   1.59414585e+00]\n",
      " [-4.13254998e-01 -1.73722344e+00  2.91658366e-01 -1.10121988e+00\n",
      "   6.05119899e-02]\n",
      " [ 3.19904277e-01  1.31962925e+00 -3.88054827e-02  4.89723547e-01\n",
      "  -4.45623017e-01]\n",
      " [-2.71353203e-01 -1.04943158e+00  4.86048865e-01 -1.28135933e+00\n",
      "  -1.46237210e+00]\n",
      " [ 8.40210860e-01  1.58710386e+00  5.05487915e-01  1.44489199e+00\n",
      "   8.19266593e-01]\n",
      " [ 6.98309065e-01 -1.04943158e+00  2.33341216e-01  4.43390988e-01\n",
      "  -1.09956736e+00]\n",
      " [-2.00402305e-01  1.20499728e+00  1.18585466e+00  9.64167666e-01\n",
      "   9.83200587e-01]\n",
      " [-4.84205896e-01 -1.01122092e+00 -5.05342680e-01 -1.28254488e+00\n",
      "  -2.62876926e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "train_mean = np.mean(X_train, axis = 0)\n",
    "train_std = np.std(X_train, axis = 0)\n",
    "X_train = (X_train - train_mean) / train_std   # Transform X = (X - muy)/sigma holds [0, 1]\n",
    "\n",
    "# When a new point sent to the model, there's no such a new dataset for test yet\n",
    "# Must use from what's been learnt to process\n",
    "X_test = (X_test - train_mean) / train_std  \n",
    "\n",
    "print(f'X_train: {X_train}\\n\\n')\n",
    "print(f'X_test: {X_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 5) (696,) (175, 5) (175,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape, X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoted vecto theta=[theta1 theta2 ... thetaN] as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build a model that use a linear function - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y_hat(W,X,b):\n",
    "    m=X.shape[0]\n",
    "    y_hat=np.zeros(m)\n",
    "    y_hat=np.dot(X,W)+b # Linear regression\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_function(y_hat,y):\n",
    "    '''\n",
    "    - Here we use MSE instead of other classification loss function because we want to see how close the ability of player is\n",
    "    between ranks (ordinal scale). For instance: y_hat = 0.9 => still iron but very close to silver => potential silver\n",
    "    - MSE punishes stronger than MAE - \"Square\" vs \"Absolute\" => MSE\n",
    "    '''\n",
    "    m = y_hat.shape[0]\n",
    "    loss=0\n",
    "    for i in range(m):\n",
    "        loss+=((y_hat[i]-y[i])**2)/m/2 # we add /2 so after derivaties, it cancels out number '2'\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_vector(W,X,b,Y):\n",
    "    m=X.shape[0]\n",
    "    n=W.shape[0]\n",
    "    dW=np.zeros(n)\n",
    "    db=0\n",
    "    y_hat=calculate_y_hat(W,X,b)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dW[j]+=(y_hat[i]-Y[i])*X[i][j]/m\n",
    "        db+=(y_hat[i]-Y[i])/m\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(*X.shape[1:]) * 0.01 # break symmetry\n",
    "b = np.random.rand()\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "iterate_time = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 100: 7.193393448732746\n",
      "Cost 200: 6.356119117470482\n",
      "Cost 300: 5.675597786790086\n",
      "Cost 400: 5.121537695088149\n",
      "Cost 500: 4.669857547519011\n",
      "Cost 600: 4.301276738988022\n",
      "Cost 700: 4.000275311138349\n",
      "Cost 800: 3.7543102539082405\n",
      "Cost 900: 3.5532142422935618\n",
      "Cost 1000: 3.3887274775841187\n",
      "Cost 1100: 3.254128854093166\n",
      "Cost 1200: 3.1439427058840668\n",
      "Cost 1300: 3.0537040083609543\n",
      "Cost 1400: 2.979769387622222\n",
      "Cost 1500: 2.919164399491132\n",
      "Cost 1600: 2.869459755660091\n",
      "Cost 1700: 2.8286707927154917\n",
      "Cost 1800: 2.7951756886329604\n",
      "Cost 1900: 2.7676488518841755\n",
      "Cost 2000: 2.745006620683053\n",
      "Cost 2100: 2.726362968382935\n",
      "Cost 2200: 2.71099335334654\n",
      "Cost 2300: 2.698305204668822\n",
      "Cost 2400: 2.687813818629535\n",
      "Cost 2500: 2.6791226694100008\n",
      "Cost 2600: 2.67190732264778\n",
      "Cost 2700: 2.6659022905125753\n",
      "Cost 2800: 2.6608902889816766\n",
      "Cost 2900: 2.6566934572718317\n",
      "Cost 3000: 2.653166180256715\n",
      "Cost 3100: 2.650189220626324\n",
      "Cost 3200: 2.64766492131741\n",
      "Cost 3300: 2.6455132826221766\n",
      "Cost 3400: 2.643668754197277\n",
      "Cost 3500: 2.642077611435871\n",
      "Cost 3600: 2.6406958095435122\n",
      "Cost 3700: 2.6394872281603283\n",
      "Cost 3800: 2.6384222353015008\n",
      "Cost 3900: 2.637476512401659\n",
      "Cost 4000: 2.636630092880324\n",
      "Cost 4100: 2.635866575332898\n",
      "Cost 4200: 2.6351724795502043\n",
      "Cost 4300: 2.6345367193705425\n",
      "Cost 4400: 2.633950171109183\n",
      "Cost 4500: 2.633405320185293\n",
      "Cost 4600: 2.632895971733412\n",
      "Cost 4700: 2.6324170135758163\n",
      "Cost 4800: 2.6319642220487722\n",
      "Cost 4900: 2.6315341029059667\n",
      "Cost 5000: 2.63112376093757\n",
      "Cost 5100: 2.6307307931000383\n",
      "Cost 5200: 2.6303532008981563\n",
      "Cost 5300: 2.6299893185343244\n",
      "Cost 5400: 2.629637753973073\n",
      "Cost 5500: 2.629297340586501\n",
      "Cost 5600: 2.6289670974695682\n",
      "Cost 5700: 2.6286461968608803\n",
      "Cost 5800: 2.6283339373879095\n",
      "Cost 5900: 2.6280297220875166\n",
      "Cost 6000: 2.6277330403426475\n",
      "Cost 6100: 2.627443453031226\n",
      "Cost 6200: 2.627160580310574\n",
      "Cost 6300: 2.6268840915647753\n",
      "Cost 6400: 2.626613697127677\n",
      "Cost 6500: 2.626349141463882\n",
      "Cost 6600: 2.626090197547509\n",
      "Cost 6700: 2.6258366622250824\n",
      "Cost 6800: 2.625588352387485\n",
      "Cost 6900: 2.6253451018071123\n",
      "Cost 7000: 2.6251067585224233\n",
      "Cost 7100: 2.6248731826729133\n",
      "Cost 7200: 2.6246442447050655\n",
      "Cost 7300: 2.624419823883902\n",
      "Cost 7400: 2.6241998070564665\n",
      "Cost 7500: 2.6239840876231053\n",
      "Cost 7600: 2.623772564680246\n",
      "Cost 7700: 2.623565142304843\n",
      "Cost 7800: 2.623361728955873\n",
      "Cost 7900: 2.6231622369727186\n",
      "Cost 8000: 2.622966582153637\n",
      "Cost 8100: 2.6227746834007952\n",
      "Cost 8200: 2.6225864624202777\n",
      "Cost 8300: 2.6224018434679373\n",
      "Cost 8400: 2.6222207531332526\n",
      "Cost 8500: 2.6220431201548546\n",
      "Cost 8600: 2.6218688752624306\n",
      "Cost 8700: 2.621697951040639\n",
      "Cost 8800: 2.6215302818115003\n",
      "Cost 8900: 2.6213658035321203\n",
      "Cost 9000: 2.621204453705395\n",
      "Cost 9100: 2.62104617130161\n",
      "Cost 9200: 2.620890896689124\n",
      "Cost 9300: 2.620738571572862\n",
      "Cost 9400: 2.6205891389393288\n",
      "Cost 9500: 2.6204425430071545\n",
      "Cost 9600: 2.6202987291823963\n",
      "Cost 9700: 2.620157644017868\n",
      "Cost 9800: 2.6200192351759397\n",
      "Cost 9900: 2.6198834513942812\n",
      "Cost 10000: 2.619750242454212\n"
     ]
    }
   ],
   "source": [
    "for k in range(iterate_time):\n",
    "    dW,db=calculate_gradient_vector(W, X_train,b,y_train)\n",
    "\n",
    "    W-=learning_rate*dW\n",
    "    b-=learning_rate*db\n",
    "\n",
    "    y_hat=calculate_y_hat(W,X_train,b)\n",
    "    loss=calculate_loss_function(y_hat, y_train)\n",
    "    \n",
    "    if (k+1) % 100 ==0:\n",
    "        print(f\"Cost {k+1}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.05120599, 1.34246105, 5.77148034, 4.85642699, 5.1925681 ,\n",
       "       4.23433207, 1.41104107, 5.10009502, 2.9205047 , 3.18088922,\n",
       "       3.37117507, 3.35835422, 1.0008349 , 5.717795  , 4.94884058,\n",
       "       1.61866313, 4.37442627, 6.10599349, 3.55476762, 5.281104  ,\n",
       "       3.64029082, 5.43802218, 2.9468311 , 3.46951161, 4.64098831,\n",
       "       3.62189446, 3.90354517, 6.52200685, 3.2193121 , 4.6595265 ,\n",
       "       5.27628706, 4.5204159 , 2.48787544, 3.23123124, 3.47290288,\n",
       "       3.45568664, 2.68544489, 3.96527383, 3.3066928 , 4.43060719,\n",
       "       3.30805531, 4.38979909, 2.28153616, 4.83955009, 4.31612725,\n",
       "       1.63242069, 5.15614808, 4.23655976, 4.38733679, 4.61872716,\n",
       "       5.53997658, 4.30501837, 4.9981773 , 4.75127666, 2.80444368,\n",
       "       3.85415807, 4.88633728, 4.13198111, 4.2683827 , 2.75923706,\n",
       "       4.07891531, 5.47684664, 3.44695924, 5.41539551, 2.50277925,\n",
       "       5.44736284, 3.26626518, 2.30335714, 0.583852  , 2.70401907,\n",
       "       3.14265356, 4.5874417 , 2.38515006, 4.04203314, 5.60300993,\n",
       "       3.28274548, 4.62094954, 4.34507799, 5.0862841 , 2.30656239,\n",
       "       4.10449155, 5.30566813, 3.50183203, 6.35519162, 4.83800843,\n",
       "       5.21319645, 5.40352698, 3.8560604 , 3.54902178, 2.68559212,\n",
       "       5.33031477, 5.32681226, 5.5145385 , 2.01096274, 5.30490583,\n",
       "       5.1123239 , 3.50583998, 2.97877428, 2.66917413, 4.24781499,\n",
       "       2.56600223, 4.39971618, 2.93055764, 5.2406305 , 2.81957379,\n",
       "       3.22284115, 5.44169402, 4.38703859, 5.84775914, 2.71743558,\n",
       "       4.43517818, 4.46096515, 3.13402687, 2.66344799, 1.9215238 ,\n",
       "       2.36376058, 4.0230522 , 3.93025085, 3.87299445, 5.374867  ,\n",
       "       5.26732166, 3.24129292, 1.55528476, 6.56289453, 4.21614867,\n",
       "       4.42384166, 3.80568219, 4.47141833, 4.08748253, 4.07418105,\n",
       "       3.98605821, 4.90909362, 2.46484394, 1.57687328, 3.17040355,\n",
       "       5.86766548, 4.20522641, 5.353709  , 5.62984773, 5.10575118,\n",
       "       3.91513707, 2.88457139, 0.14288201, 4.25093928, 1.06375557,\n",
       "       4.10901658, 7.21879288, 1.01002606, 3.84262793, 3.13402911,\n",
       "       5.10082738, 4.04698448, 4.05746599, 3.51956644, 3.53545764,\n",
       "       3.56995284, 4.52122848, 2.63639511, 3.82642583, 5.35574494,\n",
       "       2.08726969, 4.20985417, 4.92498569, 4.56640351, 3.94378304,\n",
       "       4.37480303, 2.71766154, 3.97505729, 6.43526997, 2.40875656,\n",
       "       5.69266042, 1.75926416, 4.9368275 , 2.09187652, 5.81284707])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_1 = calculate_y_hat(W,X_test,b)\n",
    "y_pred_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 0, 5, 4, 7, 6, 2, 6, 4, 3, 3, 8, 2, 8, 2, 4, 5, 8, 6, 8, 5, 8,\n",
       "       4, 1, 7, 5, 6, 5, 2, 7, 0, 4, 1, 6, 8, 6, 1, 5, 7, 0, 4, 6, 3, 8,\n",
       "       5, 2, 3, 2, 7, 6, 6, 6, 0, 8, 0, 2, 3, 0, 6, 1, 1, 2, 1, 3, 1, 6,\n",
       "       1, 5, 0, 0, 3, 3, 0, 7, 8, 4, 1, 3, 3, 1, 7, 6, 4, 5, 7, 5, 7, 8,\n",
       "       4, 0, 8, 7, 4, 0, 7, 7, 1, 4, 8, 3, 2, 1, 6, 0, 1, 0, 5, 8, 7, 2,\n",
       "       5, 1, 4, 1, 2, 3, 0, 2, 7, 6, 4, 6, 3, 6, 0, 6, 6, 7, 7, 3, 5, 1,\n",
       "       1, 4, 3, 3, 3, 3, 5, 4, 5, 5, 0, 5, 2, 8, 7, 0, 4, 0, 8, 4, 1, 8,\n",
       "       7, 1, 1, 3, 8, 6, 0, 7, 5, 4, 2, 5, 0, 2, 8, 3, 5, 4, 8, 4, 7])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(y_true, y_pred):\n",
    "    # Tổng bình phương sai số của mô hình (Residual Sum of Squares)\n",
    "    rss = np.sum((y_true - y_pred)**2)\n",
    "    # Tổng bình phương sai số so với trung bình (Total Sum of Squares)\n",
    "    tss = np.sum((y_true - np.mean(y_true))**2)\n",
    "    \n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of linear regression on train set: 5.239500484908423\n",
      "MSE of linear regression on test set: 5.255545789071955\n",
      "R-square: 0.22019820838019066\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_1 = calculate_y_hat(W,X_train,b)\n",
    "mse1_train = np.mean((y_train-y_pred_train_1)**2)\n",
    "mse1_test = np.mean((y_test-y_pred_test_1)**2)\n",
    "\n",
    "print (f'MSE of linear regression on train set: {mse1_train}')\n",
    "print (f'MSE of linear regression on test set: {mse1_test}')\n",
    "print(f'R-square: {calculate_r_squared(y_test, y_pred_test_1)}')\n",
    "# Can explain ~ 22% what's going on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3229861  -1.0444049  -0.09494827 -0.65721067  0.11165735]\n",
      "4.051583296001721\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look on the parameters\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the regression with a **quadratic function** (Polynomial regression, $deg$ = 2), with $\\theta$ equals to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train set: 4.170547021463015\n",
      "MSE on test set: 4.131100482998185\n",
      "R-square: 0.3870399598264562\n"
     ]
    }
   ],
   "source": [
    "X_design_2_train = [np.ones(X_train.shape[0])]\n",
    "# Add deg 1\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_design_2_train.append(X_train[:, i])\n",
    "# Add deg 2\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(i, X_train.shape[1]): \n",
    "        X_design_2_train.append(X_train[:, i] * X_train[:, j])\n",
    "\n",
    "X_design_2_train = np.column_stack(X_design_2_train)\n",
    "beta_2 = np.linalg.inv(X_design_2_train.T @ X_design_2_train) @ X_design_2_train.T @ y_train\n",
    "\n",
    "y_pred_train_2 = X_design_2_train @ beta_2\n",
    "\n",
    "# Predict and test\n",
    "X_design_2_test = [np.ones(X_test.shape[0])]\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_design_2_test.append(X_test[:, i])\n",
    "for i in range(X_test.shape[1]):\n",
    "    for j in range(i, X_test.shape[1]): \n",
    "        X_design_2_test.append(X_test[:, i] * X_test[:, j]) \n",
    "\n",
    "X_design_2_test = np.column_stack(X_design_2_test)\n",
    "y_pred_test_2 = X_design_2_test @ beta_2\n",
    "\n",
    "mse2_train = np.mean((y_train - y_pred_train_2) ** 2)\n",
    "mse2_test = np.mean((y_test - y_pred_test_2) ** 2)\n",
    "print(\"MSE on train set:\", mse2_train)\n",
    "print(\"MSE on test set:\", mse2_test)\n",
    "print(\"R-square:\", calculate_r_squared(y_test, y_pred_test_2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have $3-degree$ regression model with Ridge regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture even more complex interactions, we extend the model to the **third degree**. \n",
    "The parameters $\\theta$ are estimated using the **Ridge Regression** formula:\n",
    "\n",
    "$$\\theta = ( \\Phi^T \\Phi + \\lambda I )^{-1} \\Phi^T Y$$\n",
    "$$\\Phi(\\mathbf{x}) = [1, \\underbrace{x_i}_{\\text{Deg 1}}, \\underbrace{x_i x_j}_{\\text{Deg 2}}, \\underbrace{x_i x_j x_k}_{\\text{Deg 3}}]$$\n",
    "Where:\n",
    "- $\\Phi$ is the augmented design matrix including cubic terms ($x_i x_j x_k$).\n",
    "- $\\lambda$ is the regularization strength\n",
    "- $I$ is the identity matrix (with $I_{0,0} = 0$ to avoid penalizing the bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 3.8710040123972997\n",
      "MSE Test: 4.272606903987062\n",
      "R-square Test: 0.36604367037498176\n"
     ]
    }
   ],
   "source": [
    "X_design_train_3 = [np.ones(X_train.shape[0])]\n",
    "# Add deg 1\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_design_train_3.append(X_train[:, i])\n",
    "# Add deg 2\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(i, X_train.shape[1]):\n",
    "        X_design_train_3.append(X_train[:, i] * X_train[:, j])\n",
    "# Add deg 3\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(i, X_train.shape[1]):\n",
    "        for k in range(j, X_train.shape[1]):\n",
    "            X_design_train_3.append(X_train[:, i] * X_train[:, j] * X_train[:, k])\n",
    "\n",
    "X_design_train_3 = np.column_stack(X_design_train_3)\n",
    "\n",
    "# Lambda (l2 penalty) - Ridge regression needed since deg 3 causes #features increase quickly\n",
    "lambda_reg_3 = 5\n",
    "I = np.identity(X_design_train_3.shape[1])\n",
    "I[0, 0] = 0 # Usually we don't cancel out the bias\n",
    "\n",
    "XTX_3 = X_design_train_3.T @ X_design_train_3\n",
    "XTy_3 = X_design_train_3.T @ y_train\n",
    "\n",
    "beta_3 = np.linalg.inv(XTX_3 + lambda_reg_3 * I) @ XTy_3\n",
    "y_pred_train_3 = X_design_train_3 @ beta_3\n",
    "\n",
    "# Predict and test\n",
    "X_design_test_3 = [np.ones(X_test.shape[0])]\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_design_test_3.append(X_test[:, i])\n",
    "for i in range(X_test.shape[1]):\n",
    "    for j in range(i, X_test.shape[1]):\n",
    "        X_design_test_3.append(X_test[:, i] * X_test[:, j])\n",
    "for i in range(X_test.shape[1]):\n",
    "    for j in range(i, X_test.shape[1]):\n",
    "        for k in range(j, X_test.shape[1]):\n",
    "            X_design_test_3.append(X_test[:, i] * X_test[:, j] * X_test[:, k])\n",
    "\n",
    "X_design_test_3 = np.column_stack(X_design_test_3)\n",
    "\n",
    "# Predict and test\n",
    "y_pred_test_3 = X_design_test_3 @ beta_3\n",
    "\n",
    "mse_train_3 = np.mean((y_train - y_pred_train_3) ** 2)\n",
    "mse_test_3 = np.mean((y_test - y_pred_test_3) ** 2)\n",
    "\n",
    "print(f\"MSE on train set: {mse_train_3}\")\n",
    "print(f\"MSE on test set: {mse_test_3}\")\n",
    "print(f\"R-square: {calculate_r_squared(y_test, y_pred_test_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the MSE of all model below, we believe that with the complexity of higher degree model, the data can be fit more than ever. We can say that quadratic regression model is the optimal choice here ($R^2$ $\\approx$ 0.38; $ MSE_{test}$ $\\approx$ 4.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.51964301e-05 -2.56017048e-04  5.27056521e-05  8.10163661e-04\n",
      "  1.30950222e-03  9.24688198e-04  2.98320456e-03 -5.79983197e-04\n",
      " -8.79864990e-03  1.13369685e-04 -7.38619749e-03  6.42071561e-04\n",
      "  2.21255066e-03 -5.29638996e-05  5.28915144e-04  2.79390672e-02\n",
      " -4.49616577e-05  6.80554326e-04 -8.17828897e-08 -3.30893718e-06\n",
      "  3.70786331e-04]\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save weights and bias on the whole dataset to deploy\n",
    "X_design_final = [np.ones(X.shape[0])]\n",
    "for i in range(X.shape[1]):\n",
    "    X_design_final.append(X[:, i])\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i, X.shape[1]):\n",
    "        X_design_final.append(X[:, i] * X[:, j])\n",
    "X_design_final = np.column_stack(X_design_final)\n",
    "\n",
    "theta_final = np.linalg.pinv(X_design_final.T @ X_design_final) @ X_design_final.T @ y\n",
    "print(theta_final)\n",
    "np.save('model.npy', theta_final)\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
